---
title: "Project2"
subtitle: "Gaussian random field with application of INLA"
author: "Yaolin Ge, Florian Beiser"
output:
  pdf_document:
    toc: yes
    toc_depth: 2
  #   toc: true
  #   toc_depth: 3
  #   number_sections: true
  html_document:
    toc: yes
    toc_float: yes
    code_download: yes
    toc_depth: 3
  ioslides_presentation: default
  beamer_presentation:
    slide_level: 1
    keep_tex: yes
---

```{r setup, include=TRUE,echo=FALSE}
suppressPackageStartupMessages(library(knitr))
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE, error = FALSE)
```

```{r load libraries, echo = F}
library(lattice)
library(viridisLite)
coul <- viridis(100)
```

# Part I Multivariate normal distribution
Let $\boldsymbol{x} = (x_1, \dots, x_n), n = 100$ be multivariate normal distributed with $E(x_i) = 0, Var(x_i) = 1$, and $Corr(x_i, x_j) = e^{-0.1|i - j|}$

a) Compute and image the covariance matrix $\boldsymbol{\Sigma}$ of $\boldsymbol{x}$

b) Find the lower Cholesky factor $\boldsymbol{L}$, such that $\boldsymbol{L}\boldsymbol{L}^T = \boldsymbol{\Sigma}$, of this covariance matrix, and image.

c) Sample $\boldsymbol{x} = \boldsymbol{Lz}$, where $\boldsymbol{z}$ is a length n random vector of independent standard normal variables. Plot the sample.

d) Find the precision matrix $\boldsymbol{Q}$ of the covariance matrix, and compute the lower Cholesky factor $\boldsymbol{L}_Q$, such that $\boldsymbol{L}_Q\boldsymbol{L}_Q^T = \boldsymbol{Q}$, of this matrix. Image these matrices and compare them to the images obtained in a) and b)

e) Sample $\boldsymbol{x}$ by solving $\boldsymbol{L}_Q^T\boldsymbol{x} = \boldsymbol{z}$, where $\boldsymbol{z}$ is a length n random vector of independen standard normal variables. Plot the sample.

f) Permute the ordering of variables in $\boldsymbol{x}$, and redo the exercises. 


***

## Solution to Part I

### a) 
Given that $\boldsymbol{\Sigma} = e^{-0.1|i - j|}$. The covariance matrix can be expressed as follows:
\begin{equation*}
    \Sigma = \begin{pmatrix} 
    1 &  e^{- 0.1 h_{12}} &\dots & e^{- 0.1 h_{1n}} \\
    e^{- 0.1 h_{21}} & 1 & \dots & e^{- 0.1 h_{2n}} \\
    \vdots & \vdots & \ddots & \vdots \\
    e^{- 0.1 h_{n1}} & e^{- 0.1 h_{n2}} & \dots & 1
    \end{pmatrix}
\end{equation*}

```{r covariance matrix, echo = F, fig.align='center'}
# sizes
n <- 100

# define regular grid of locations
sites1v <- array((1:n),c(n,1))

# Prior mean
m <- 0
# compute East and North distances on grid
ww <- array(1,c(n,1))

# determine the distance matrix
H <- abs(sites1v%*%t(ww)-ww %*% t(sites1v))

# Exponential covariance model
Sigma <- exp(-0.1*H)

# Plot the covariance matrix
levelplot(t(Sigma), 
          col.regions = coul, main = "Covariance matrix",
          ylim=c(100,1))
```
### b) 
According to the cholesky decomposition rule, $\boldsymbol{L}$ is the lower triangular matrix for $\boldsymbol{\Sigma}$, it can be easily computed from R using `L = chol(Sigma)`. It is then plotted as below. 

```{r Cholesky, echo = F, fig.align='center'}
# Using R function to calculate Cholesky decomposition
L <- chol(Sigma)
levelplot(t(L), ylim=c(100,1), col.regions = coul, main = "Lower triangular matrix")

# FB: Attention!! chol() gives what is L^t in our exercise sheet, i.e. L=chol(Sigma) is an upper but NOT lower matrix!
```


### c) 
Sample using $\boldsymbol{x} = \boldsymbol{L}\boldsymbol{z}$ transforms the zero-mean, standard normal random variales to the random variables with the desired covariance matrix. 

```{r, random samples, fig.align='center'}
z = rnorm(100)
x = L %*% z
plot(x, main = "Random samples given the covariance")
```


### d) 
The precision matrix $\boldsymbol{Q}$ is the inverse of the covariance matrix $\boldsymbol{\Sigma}$, it is computed using `Q = solve(Sigma)` in R. The three matrices are thereby depicted as follows. Since the covariance matrix is not singular, given that the it belongs to the Matern family, thus it is analytically guaranteed to have positive definite property. Therefore, both precision matrix and the lower triangular precision matrix exist. 

```{r comp between three matrices, fig.show = "hold", out.width="50%"}
Q <- solve(Sigma)
LQ <- chol(Q)
par(mar = c(4,4,.1, .1))
# par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
levelplot(Sigma, col.regions = coul, main = "Covariance matrix")
levelplot(L, col.regions = coul, main = "Lower triangular covariance matrix")
levelplot(Q, col.regions = coul, main = "Precision matrix")
levelplot(LQ, col.regions = coul, main = "Lower triangular precision matrix")
# levelplot(LQ, col.regions = coul, main = "Lower triangular matrix")
```


### e) 
Similarly, the expected random samples can be generated using the inversion of the above formula, thus $\boldsymbol{L}_Q^T\boldsymbol{x} = \boldsymbol{z}$

```{r, sample in inversion way, fig.align='center'}
z = rnorm(100)
x = solve(t(LQ), z)
plot(x, main = "Random samples using inversion rule")
```

### f) 
Permute $\boldsymbol{x}$ to make randomise the ordering of the grid, the asscociated covariance matrix can be thereby modified in a sparse way. 

```{r, permutation, fig.align='center'}
n_per = 3
Sigma_per <- list()
L_per <- list()
Q_per <- list()
LQ_per <- list()
for (i in c(1:n_per)){
  sites1v_per <- array(sample(sites1v, size = n, replace = FALSE), c(n,1))
  # determine the distance matrix
  H_per <- abs(sites1v_per%*%t(ww)-ww %*% t(sites1v_per))
  # Exponential covariance model
  Sigma_temp <- exp(-0.1*H_per)
  Sigma_per[[i]] <- Sigma_temp
  # L matrix
  L_temp <- chol(Sigma_temp)
  L_per[[i]] <- L_temp
  
  z = rnorm(100)
  x_per = L_temp %*% z
  plot(x_per, main = paste(i, "Permuted random samples given the covariance"))
  
  Q_temp <- solve(Sigma_temp)
  LQ_temp <- chol(Q_temp)
  Q_per[[i]] <- Q_temp
  LQ_per[[i]] <- LQ_temp
  z = rnorm(100)
  x_per = solve(t(LQ_temp), z)
  plot(x_per, main = paste(i, "Permuted random samples using inversion rule"))
}


```

```{r, cont plotting 1, fig.show = "hold", out.width="50%"}
i = 1
par(mar = c(4, 4,.1, .1))
# par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
levelplot(Sigma_per[[i]], col.regions = coul, main = paste(i, "Permuted covariance matrix"))
levelplot(L_per[[i]], col.regions = coul, main = paste(i,"Permuted lower triangular covariance matrix"))
levelplot(Q_per[[i]], col.regions = coul, main = paste(i,"Permuted precision matrix"))
levelplot(LQ_per[[i]], col.regions = coul, main = paste(i,"Permuted lower triangular precision matrix"))
```

```{r, cont plotting 2, fig.show = "hold", out.width="50%"}
i = 2
par(mar = c(4, 4,.1, .1))
# par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
levelplot(Sigma_per[[i]], col.regions = coul, main = paste(i, "Permuted covariance matrix"))
levelplot(L_per[[i]], col.regions = coul, main = paste(i,"Permuted lower triangular covariance matrix"))
levelplot(Q_per[[i]], col.regions = coul, main = paste(i,"Permuted precision matrix"))
levelplot(LQ_per[[i]], col.regions = coul, main = paste(i,"Permuted lower triangular precision matrix"))
```

```{r, cont plotting 3, fig.show = "hold", out.width="50%"}
i = 3
par(mar = c(4, 4,.1, .1))
# par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
levelplot(Sigma_per[[i]], col.regions = coul, main = paste(i, "Permuted covariance matrix"))
levelplot(L_per[[i]], col.regions = coul, main = paste(i,"Permuted lower triangular covariance matrix"))
levelplot(Q_per[[i]], col.regions = coul, main = paste(i,"Permuted precision matrix"))
levelplot(LQ_per[[i]], col.regions = coul, main = paste(i,"Permuted lower triangular precision matrix"))
```



# Part II Gaussian random fields and Kriging

The purpose of this computer exercise is to give an introduction to parameter estima- tion and kriging for Gaussian random field models for spatial data.

We assume the following observation model on the unit square:

\begin{equation*}
    y(\boldsymbol{s}_j) = x(\boldsymbol{s}_j) + \epsilon_j, \ \ \  j = 1, \dots, N,
\end{equation*}

where $\epsilon_j ~ \mathop{N}(0, \tau^2)$ are independent measurement noise terms. Further, consider a MatÃ©rn covariance function for the Gaussian random field $\boldsymbol{x(s)}$:

\begin{equation*}
    Cov(x(\boldsymbol{s}_i), x(\boldsymbol{s}_j)) = \Sigma_{i, j} = \sigma^2(1 + \phi h)\exp(-\phi h),
\end{equation*}

where $h$ denotes the Euclidean distance between the two sites $\boldsymbol{s}_i$ and $\boldsymbol{s}_j$.

We assume the mean increases with east and north coordinates as follows: $\mu_j = \alpha((s_{j1} - 0.5) + (s_{j2} - 0.5))$, for site $\boldsymbol{s}_j = (s_{j1}, s_{j2})$ on the unit square. 

*** 

## 2.1 Simulation

Simulate $N = 200$ random sites in the unit square and plot them. Form the covariance matrix using $\sigma = 1, \phi = 10, \tau = 0.05$. Take its Cholesky decomposition and simulate dependent zero-mean Gaussian data variables, then add the mean using $\alpha = 1$. Plot your observations.

```{r GP simulation}

Matern_cov <- function(sigma, phi, t){
    # param sigma: scaling coef
    # param eta: range coef
    # param t: distance matrix
    # return: matern covariance
    return(sigma ^ 2 * (1 + phi * t) * exp(-phi * t))
}

plotf <- function(v, string){
  vv <- v
  dim(vv) <- c(n1, n2)
  levelplot(vv, col.regions = coul, main = string)
}

# Setup the grid
n1 = 25 # number of grid points along east direction
n2 = 25 # number of grid points along north direction
n = n1 * n2 # total number of grid points

dn1 = 1/n1
dn2 = 1/n2
sites1 = array(seq(0, 1, dn1), c(n1, 1))
sites2 = array(seq(0, 1, dn2), c(n2, 1))
# sites1 = array(seq(1, n1), c(n1, 1))
# sites2 = array(seq(1, n2), c(n2, 1))
ww1 = rep(1, n1)
ww2 = rep(1, n2)
sites1m = sites1 %*% t(ww1) # sites1m is the matrix version of sites1
sites2m = ww2 %*% t(sites2)

sites1v = matrix(sites1m, nrow = n, ncol = 1)
sites2v = matrix(sites2m, nrow = n, ncol = 1)

plot(sites1v, sites2v)

# Compute the distance matrix
ddE = sites1v %*% matrix(rep(1, n), nrow = 1, ncol = n) - matrix(rep(1, n), nrow = n, ncol = 1) %*% t(sites1v)
dd2E = ddE * ddE
ddN = sites2v %*% matrix(rep(1, n), nrow = 1, ncol = n) - matrix(rep(1, n), nrow = n, ncol = 1) %*% t(sites2v)
dd2N = ddN * ddN
t = sqrt(dd2E + dd2N)
levelplot(t, col.regions = coul, main = "Distance matrix")

# Simulate the initial random field
alpha = 1.0 # beta as in regression model
sigma = 1.0  # scaling coef in matern kernel
phi = 10 # range coef in matern kernel
# eta = 10 # range coef in matern kernel
tau = .05 # iid noise

beta1 = -alpha
beta2 = alpha
beta3 = alpha

BETA_TRUE = matrix(c(beta1, beta2, beta3), nrow = 3, ncol = 1)
THETA_TRUE = matrix(c(sigma, phi, tau), nrow = 3, ncol = 1)

Sigma = Matern_cov(sigma, phi, t)  # matern covariance

L = chol(Sigma) # lower L
x = t(L) %*% rnorm(n) # sample from zero mean random variables

H = array(c(rep(1, n), sites1v, sites2v), dim = c(n, 3)) # design matrix
mu_prior = H %*% BETA_TRUE
plotf(mu_prior, "prior mean")
mu_real = mu_prior + x
plotf(mu_real, "realisation of the grf")

# sampling from realisations
M = 200
Fmatrix = matrix(0, M, n)
ind = sample(n, size = M, replace = FALSE)
for (i in c(1:M)){
  Fmatrix[i, ind[i]] = TRUE
}
G = Fmatrix %*% H
y_sampled = Fmatrix %*% mu_real + tau * rnorm(M, 1)
x_ind = sites1v[ind]
y_ind = sites2v[ind]

plot(x_ind, y_ind, cex = abs(y_sampled), main = "Random samples in the field, circle size indicates the relative value")

```

***

## 2.2 Paramter estimation

We will now use the simulated data to estimate the model parameters $\alpha, \sigma^2, \tau^2, \phi$ using maximum likelihood estimation. Iterate between the update for the mean parameter, and updating the covariance parameters. Monitor the likelihood function at each step of the algorithm to check convergence.

```{r, parameter estimation}
#%%
C_matrix <- function(theta){
  sigma = theta[1]
  phi = theta[2]
  tau = theta[3]
  Sigma = Matern_cov(sigma, phi, t)
  C = Fmatrix %*% Sigma %*% t(Fmatrix) + diag(M) * tau^2
  return(C)
}


dC_dsigma <- function(theta){
  # sigma = theta$sigma
  # phi = theta$phi
  # tau = theta$tau
  sigma = theta[1]
  phi = theta[2]
  tau = theta[3]
  Km = Matern_cov(1.0, phi, t) # t here is the distance matrix, H is the design matrix, similar to X
  dC_dsgm = Fmatrix %*% Km %*% t(Fmatrix)
  return(dC_dsgm)
}

dC_dphi <- function(theta){
  sigma = theta[1]
  phi = theta[2]
  tau = theta[3]
  Kn = sigma ^ 2 * (-phi * t) * exp(-phi * t)
  return(Fmatrix %*% Kn %*% t(Fmatrix))
}

dC_dtau <- function(theta){
  return(diag(M))
}

# Use fisher scoring to find MLE parameters
# beta = np.zeros([3, 1])
beta = matrix(c(-2.1, 3.1, .9), nrow = 3, ncol = 1)
theta = rbind(.245, 9.3, .003)
MAX_ITER = 5
No_iter = 0
epsilon = 10
Beta = matrix(0, nrow = MAX_ITER, ncol = 3)
Likelihood = matrix(0, nrow = MAX_ITER, ncol = 1)

while (No_iter < MAX_ITER & epsilon > .0001){
    C = C_matrix(theta)
    beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
    # beta = np.linalg.solve(np.dot(G.T, np.linalg.solve(C, G)), np.dot(G.T, np.linalg.solve(C, y_sampled)))
    Beta[No_iter, ] = t(beta)
    z = y_sampled - G %*% beta
    lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
    Likelihood[No_iter, ] = lik

    # Find dC*/dtheta
    dC_dSgm = dC_dsigma(theta)
    dC_dPhi = dC_dphi(theta)
    dC_dTau = dC_dtau(theta)

    u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm))) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z))
    u_eta = -1 / 2 * sum(diag(solve(C, dC_dPhi))) + 1 / 2 * t(z) %*% solve(C, dC_dPhi %*% solve(C, z))
    u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau))) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z))

    u = rbind(u_sigma, u_eta, u_tau)

    V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
    V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
    V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
    V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
    V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
    V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
    V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
    V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
    V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))

    V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
    # print(lik)
    # print(No_iter)
    theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
    epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
    theta = theta_new
    print(paste(epsilon , " , iter no is ", No_iter))
    No_iter = No_iter + 1
}
# print(beta)
# print(BETA_TRUE)
# print(theta)
# print(THETA_TRUE)

plot(c(1:No_iter), Likelihood, main = "likelihood function")

# alphah = (sum(np.abs(beta)) / 3).squeeze()
thetah = theta
sigmah = theta[1]
phih = theta[2]
tauh = theta[3]
betah = beta
beta1 = beta[1]
beta2 = beta[2]
beta3 = beta[3]

# print('Estimated sigma is ', sigmah, "\nEstimated eta is ", etah, \
#       "\nEstimated tau is ", tauh, "\nEstimated alpha is ", alphah)
print(cat("\nEstimated sigma is ", round(sigmah, digits = 3), "; True sigma is ", THETA_TRUE[1], 
            "\nEstimated phi is ", round(phih, digits = 2), "; True phi is ", THETA_TRUE[2], 
            "\nEstimated tau is ", round(tauh, digits = 5), "; True tau is ", THETA_TRUE[3], 
            "\nEstimated beta1 is ", round(beta1, digits = 2), "; True beta1 is ", BETA_TRUE[1], 
            "\nEstimated beta2 is ", round(beta2, digits = 2), "; True beta2 is ", BETA_TRUE[2], 
            "\nEstimated beta3 is ", round(beta3, digits = 2), "; True beta3 is ", BETA_TRUE[3]))


```


*** 

## 2.3 Kriging

We will now use the estimated model parameters to perform kriging prediction. Predict variables $x(s)$, where predictions sites lie on a regular grid of size 25x25 for the unit square. Visualize the Kriging surface and the prediction standard error. Compare with the true field.

```{r, kriging}
Sigmah = Matern_cov(sigmah, phih, t) # estimated covariance matrix
Lh = chol(Sigmah)
mh = H %*% betah + Lh %*% rnorm(n)
Ch = C_matrix(thetah)
xp = mh + Sigmah %*% t(Fmatrix) %*% solve(C, (y_sampled - Fmatrix %*% mh))
plotf(xp, "posterior mean")
Sigmap = Sigmah - Sigmah %*% t(Fmatrix) %*% solve(Ch, Fmatrix %*% Sigmah)
# plotf(Sigmap, "posterior covariance")
estd = sqrt(diag(Sigmap))
plotf(estd, "posterior std")
MSE = sqrt(sum(abs(xp - mu_real) ** 2) / n)
print(paste("The prediction error is ", MSE))
```

***

# Part III Integrated nested Laplace Approximations (INLA)

In the last part of this exercise, we explore the `R-INLA` package along two examples.

## 3.1 Simple Linear Regression

First, we analyse the ski jumping data set using a linear regression model, which can be phrased as Latent Gaussian model suitable for INLA. Therefore, we start with loading the INLA package and exploring the dataset. 

```{r INLA1, echo=FALSE}
# 1 Load R-INLA package
library(INLA)

# 2 Load and save SkiPump-dataset
skiData = read.table("SkiJump.txt", header=TRUE)
```

The ski jumping data set contains 26 observations of measured lengths in ski jumping competitions (in meters) between the years 1961 and 2011. 

```{r INLA2, echo=FALSE, fig.cap="\\label{fig:skiData}Visualisation of the ski jumping data set"}
# 3 Visualisation
plot(skiData$Year, skiData$Length,
     xlab="year", ylab="length",
     main="Ski jumping data")
```

In Figure \ref{fig:skiData} we depict the 26 observations given their year. We observe a clear (almost linear) trend in the measured jumping lengths to increase with the years.

Suitable for this model assumption on the data, we use linear regression approach for the statistical modelling of this data, where the years $x_i$ are the covariates and the lengths $y_i$ are the responses for $i=1,\dots,26$:
\begin{equation*}
\mathbb{E}[y_i] = \mu + \beta x_i,\quad \mathbb{Var}[y_i]=\tau^{-1}.
\end{equation*}

This can be posed as a latent Gaussian model suited for the INLA framework.

1.    The response depends on the linear predictors $\eta$ as $y|x,\theta = \Pi \pi(y_i|\eta_i,\tau)$ with Gaussian likelihood $\pi(y_i|\eta_i,\tau)\sim\mathcal{N}(\eta_i,\tau^{-2})$ 
2.    The parameters $\mu$ and $\beta$ of the linear predictor $eta_i=\mu+x_i\beta$ are independent Gaussian with a fixed huge variance and mean zero. Note that no additional hyperparameter is introduced here.
3.    The model's hyperparameter $\tau$ is only one-dimensional and is equipped with a Gaussian prior by default with out specification. 

The distributions which are not specified in detail here use default settings in the `R-INLA` package, which are naturally compatible with the LGM construction.

```{r INLA3, echo=FALSE}
# 4 Linear regression using R-INLA
res = inla(Length ~ Year, data=skiData,
           control.predictor = list(compute=TRUE))

# Post processing
summary(res)
```
The INLA run generates posterior estimates for the fixed effects $\mu$ and $\beta$, which will be investigated blow. In the summary, we read the precision of the distribution for the hyperparameter $\tau$ which is rather small, meaning that the variance will be rather big. However, in a ski jumping competition we can expect a variance of several meters such that this is reasonable.


```{r INLA4, echo=FALSE, fig.width=6, fig.height=4, fig.cap="\\label{fig:INLApredictor}Linear regression with INLA"}
# Plotting fitted values (equal to linear predictor)
plot(skiData$Year, skiData$Length, col="red",
     xlab="year", ylab="length",
     main="Prediction for the data")
lines(skiData$Year, res$summary.linear.predictor$mean)
lines(skiData$Year, res$summary.linear.predictor$`0.025quant`, lty=2)
lines(skiData$Year, res$summary.linear.predictor$`0.975quant`, lty=2)
```

In Figure \ref{fig:INLApredictor} we see that the linear trend is really well captured by the model (black line), whereas the 95% credibility interval (dashed line) does not cover all data points.


```{r INLA5, echo=FALSE, fig.width=12, fig.height=8, fig.cap="\\label{fig:INLAmarginals}Posterior marginals for selected effects"}
# Plotting marginal distribution
par(mfrow=c(2,2))
plot(inla.smarginal(res$marginals.fixed[[1]]),
     xlab="mu", ylab="density",
     main="Posterior marginal for mu")

plot(inla.smarginal(res$marginals.fixed[[2]]),
     xlab="beta", ylab="density",
     main="Posterior marginal for beta")

plot(inla.smarginal(res$marginals.hyperpar[[1]]),
     xlab="tau", ylab="density",
     main="Posterior for the hyperparameter tau")

plot(inla.smarginal(
        inla.tmarginal(function(x)1/sqrt(x), 
            res$marginals.hyperpar[[1]])), 
     xlab="sigma", ylab="density",
     main="Posterior marginal for transformed hyperparameter sigma")

# Printing estimate
print("Estimates for sigma:")
inla.zmarginal(inla.tmarginal(function(x)1/sqrt(x),res$marginals.hyperpar[[1]]))
```

In Figure \ref{fig:INLAmarginals}, we depict the marginal posterior distributions for all variables of interest. In particular, the transformed marginal analysis confirms our interpretation that the variance in the magnitude of several meters (roughly around 4 meters), but again this is very reasonable for this application. 


## 3.2 GLMM with random effects

Last, we use INLA to analyse the "Seeds" data set. This data concerns the portion of seeds that germinated on a sample set of 21 plates. The plates are equipped with one of two types of seeds and one of two types of extracts. The characteristics of plate $i$ are described by the covariates $x_{1,i}$ which is the type of seed and $x_{2,i}$ which is the type of root extract - these covariates are either 0 or 1 for the different possibilies. Then the number of germinated seeds $r_i$ on plate $i$ is counted in contrast to the number of total seeds $n_i$ on that plate. Having $p_i$ as the probability of germination on plate $i$, a binomial model for this example is
\begin{align*}
r_i &\sim Binomial(p_i,n_i)\\
\text{logit}(p_i) &= a_0 + a_1x_{1,i} + a_2x_{2,i}+\varepsilon_i
\end{align*}
where $\varepsilon_i$ is some iid noise. As above non-specified prior- and hyperparameter-distributions use the default of `R-INLA`. This model is then again implemented in `R-INLA`.  

```{r GLMM1, echo=FALSE}
library(INLA)
# Load dataset
data(Seeds)

# Prepare INLA call
data = Seeds
formula = r ~ x1 + x2 + f(plate, model="iid")

print("The x1 covariate:")
print(data$x1)
print("The x2 covariate:")
print(data$x2)

# Call INLA
res = inla(formula=formula, data=data,
           family="binomial", Ntrials=n,
           control.family = list(link = "logit"), 
           control.predictor = list(compute=TRUE))

# Post processing
summary(res)
```

Remark the structured pattern in the covariates. Furthermore, here the precision is rather higher yielding a small variance in the estimate.

```{r GLMM2, echo=FALSE, fig.width=12, fig.height=4, fig.cap="\\label{fig:GLMMmarginals}Posterior marginals for selected effects"}
# Plot marginals for fixed effect
par(mfrow=c(1,3))
plot(inla.smarginal(res$marginals.fixed[[1]]),
     xlab="a0", ylab="density",
     main="Posterior marginal for a0")

plot(inla.smarginal(res$marginals.fixed[[2]]),
     xlab="a1", ylab="density",
     main="Posterior marginal for a1")

plot(inla.smarginal(res$marginals.fixed[[3]]),
     xlab="a2", ylab="density",
     main="Posterior marginal for a2")
```

In Figure \ref{fig:GLMMmarginals}, we see rather confident estimates for the fixed effects of the model. In particular, we note that the effect $a_2$ will dominate the model since we only have 0 and 1 values for the covariates. The effect $a_0$ is common for all plates and the absolute value of $a_1$ for the seed type is a way smaller than the one for the root extract.


```{r GLMM3, echo=FALSE, fig.width=12, fig.height=4, fig.cap="\\label{fig:GLMMpredict}GLMM with INLA"}
# Plot linear predictor and fitted values
par(mfrow=c(1,2))
plot(res$summary.linear.predictor$mean,
     xlab="plate", ylab="logit",
     main="Posterior marginal mean for logits")
     
plot(data$plate, data$r/data$n, col="red",
     xlab="plate", ylab="p", ylim=c(0,1),
     main="Posterior marginal mean for p")
lines(res$summary.fitted.values$mean)
lines(res$summary.fitted.values$`0.025quant`, lty=2)
lines(res$summary.fitted.values$`0.975quant`, lty=2)
```

In Figure \ref{fig:GLMMpredict}, we recognize a clear pattern with high values when the root extract with value 1 is chosen. The choice of the seed only has minor influence on the prediction. Moreover, the variance of this model cannot cover all data points, which is a common issue in binomial models with few data though. 