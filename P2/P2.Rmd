---
title: "Project2"
subtitle: "Gaussian random field with application of INLA"
author: "Florian Beiser, Yaolin Ge"
output:
  html_document:
    toc: yes
    toc_float: yes
    code_download: yes
    toc_depth: 3
  ioslides_presentation: default
  beamer_presentation:
    slide_level: 1
    keep_tex: yes
  pdf_document:
    toc: yes
    toc_depth: 2
---

```{r setup, include=TRUE,echo=FALSE}
suppressPackageStartupMessages(library(knitr))
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE, error = FALSE)
```

```{r load libraries, echo = F}
library(lattice)
library(viridisLite)
coul <- viridis(100)
```

# Part I Multivariate normal distribution
Let $\boldsymbol{x} = (x_1, \dots, x_n), n = 100$ be multivariate normal distributed with $E(x_i) = 0, Var(x_i) = 1$, and $Corr(x_i, x_j) = e^{-0.1|i - j|}$

a) Compute and image the covariance matrix $\boldsymbol{\Sigma}$ of $\boldsymbol{x}$

b) Find the lower Cholesky factor $\boldsymbol{L}$, such that $\boldsymbol{L}\boldsymbol{L}^T = \boldsymbol{\Sigma}$, of this covariance matrix, and image.

c) Sample $\boldsymbol{x} = \boldsymbol{Lz}$, where $\boldsymbol{z}$ is a length n random vector of independent standard normal variables. Plot the sample.

d) Find the precision matrix $\boldsymbol{Q}$ of the covariance matrix, and compute the lower Cholesky factor $\boldsymbol{L}_Q$, such that $\boldsymbol{L}_Q\boldsymbol{L}_Q^T = \boldsymbol{Q}$, of this matrix. Image these matrices and compare them to the images obtained in a) and b)

e) Sample $\boldsymbol{x}$ by solving $\boldsymbol{L}_Q^T\boldsymbol{x} = \boldsymbol{z}$, where $\boldsymbol{z}$ is a length n random vector of independen standard normal variables. Plot the sample.

f) Permute the ordering of variables in $\boldsymbol{x}$, and redo the exercises. 


***

## Solution to Part I

### a) 
Given that $\boldsymbol{\Sigma} = e^{-0.1|i - j|}$. The covariance matrix can be expressed as follows:
\begin{equation*}
    \Sigma = \begin{pmatrix} 
    1 &  e^{- 0.1 h_{12}} &\dots & e^{- 0.1 h_{1n}} \\
    e^{- 0.1 h_{21}} & 1 & \dots & e^{- 0.1 h_{2n}} \\
    \vdots & \vdots & \ddots & \vdots \\
    e^{- 0.1 h_{n1}} & e^{- 0.1 h_{n2}} & \dots & 1
    \end{pmatrix}
\end{equation*}

```{r covariance matrix, echo = F, fig.align='center'}
# sizes
n <- 100

# define regular grid of locations
sites1v <- array((1:n),c(n,1))

# Prior mean
m <- 0
# compute East and North distances on grid
ww <- array(1,c(n,1))

# determine the distance matrix
H <- abs(sites1v%*%t(ww)-ww %*% t(sites1v))

# Exponential covariance model
Sigma <- exp(-0.1*H)

# Plot the covariance matrix
levelplot(Sigma, col.regions = coul, main = "Covariance matrix")
```
### b) 
According to the cholesky decomposition rule, $\boldsymbol{L}$ is the lower triangular matrix for $\boldsymbol{\Sigma}$, it can be easily computed from R using `L = chol(Sigma)`. It is then plotted as below. 

```{r Cholesky, echo = F, fig.align='center'}
L <- chol(Sigma)
levelplot(L, col.regions = coul, main = "Lower triangular matrix")
```


### c) 
Sample using $\boldsymbol{x} = \boldsymbol{L}\boldsymbol{z}$ transforms the zero-mean, standard normal random variales to the random variables with the desired covariance matrix. 

```{r, random samples, fig.align='center'}
z = rnorm(100)
x = L %*% z
plot(x, main = "Random samples given the covariance")
```


### d) 
The precision matrix $\boldsymbol{Q}$ is the inverse of the covariance matrix $\boldsymbol{\Sigma}$, it is computed using `Q = solve(Sigma)` in R. The three matrices are thereby depicted as follows. Since the covariance matrix is not singular, given that the it belongs to the Matern family, thus it is analytically guaranteed to have positive definite property. Therefore, both precision matrix and the lower triangular precision matrix exist. 

```{r comp between three matrices, fig.show = "hold", out.width="50%"}
Q <- solve(Sigma)
LQ <- chol(Q)
par(mar = c(4,4,.1, .1))
# par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
levelplot(Sigma, col.regions = coul, main = "Covariance matrix")
levelplot(L, col.regions = coul, main = "Lower triangular covariance matrix")
levelplot(Q, col.regions = coul, main = "Precision matrix")
levelplot(LQ, col.regions = coul, main = "Lower triangular precision matrix")
# levelplot(LQ, col.regions = coul, main = "Lower triangular matrix")
```


### e) 
Similarly, the expected random samples can be generated using the inversion of the above formula, thus $\boldsymbol{L}_Q^T\boldsymbol{x} = \boldsymbol{z}$

```{r, sample in inversion way, fig.align='center'}
z = rnorm(100)
x = solve(t(LQ), z)
plot(x, main = "Random samples using inversion rule")
```

### f) 
Permute $\boldsymbol{x}$ to make randomise the ordering of the grid, the asscociated covariance matrix can be thereby modified in a sparse way. 

```{r, permutation, fig.align='center'}
sites1v_per <- array(sample(sites1v, size = n, replace = FALSE), c(n,1))
# determine the distance matrix
H_per <- abs(sites1v_per%*%t(ww)-ww %*% t(sites1v_per))
# Exponential covariance model
Sigma_per <- exp(-0.1*H_per)
# L matrix
L_per <- chol(Sigma_per)

z = rnorm(100)
x_per = L_per %*% z
plot(x_per, main = bquote("Permuted random samples given the covariance"))


Q_per <- solve(Sigma_per)
LQ_per <- chol(Q_per)
z = rnorm(100)
x_per = solve(t(LQ_per), z)
plot(x_per, main = "Permuted random samples using inversion rule")

```

```{r, cont plotting, fig.show = "hold", out.width="50%"}
par(mar = c(4,4,.1, .1))
# par(mfrow=c(1,3), mar=c(4,4,4,1), oma=c(0.5,0.5,0.5,0))
levelplot(Sigma_per, col.regions = coul, main = "Permuted covariance matrix")
levelplot(L_per, col.regions = coul, main = "Permuted lower triangular covariance matrix")
levelplot(Q_per, col.regions = coul, main = "Permuted precision matrix")
levelplot(LQ_per, col.regions = coul, main = "Permuted lower triangular precision matrix")
```

# Part II Gaussian random fields and Kriging

The purpose of this computer exercise is to give an introduction to parameter estima- tion and kriging for Gaussian random field models for spatial data.

We assume the following observation model on the unit square:

\begin{equation*}
    y(\boldsymbol{s}_j) = x(\boldsymbol{s}_j) + \epsilon_j, \ \ \  j = 1, \dots, N,
\end{equation*}

where $\epsilon_j ~ \mathop{N}(0, \tau^2)$ are independent measurement noise terms. Further, consider a Matérn covariance function for the Gaussian random field $\boldsymbol{x(s)}$:

\begin{equation*}
    Cov(x(\boldsymbol{s}_i), x(\boldsymbol{s}_j)) = \Sigma_{i, j} = \sigma^2(1 + \phi h)\exp(-\phi h),
\end{equation*}

where $h$ denotes the Euclidean distance between the two sites $\boldsymbol{s}_i$ and $\boldsymbol{s}_j$.

We assume the mean increases with east and north coordinates as follows: \mu_j = \alpha((s_{j1} - 0.5) + (s_{j2} - 0.5)), for site $\boldsymbol{s}_j = (s_{j1}, s_{j2})$ on the unit square. 

*** 

## 2.1 Simulation

Simulate N = 200 random sites in the unit square and plot them. Form the covari- 222
ance matrix using σ = 1, φ = 10 and τ = 0.05 . Take its Cholesky decomposition and simulate dependent zero-mean Gaussian data variables, then add the mean using α = 1. Plot your observations.

```{r, GP simulation}
Matern_cov <- function(sigma, phi, t){
    # param sigma: scaling coef
    # param eta: range coef
    # param t: distance matrix
    # return: matern covariance
    return(sigma ^ 2 * (1 + phi * t) * exp(-phi * t))
}


# Setup the grid
n1 = 25 # number of grid points along east direction
n2 = 25 # number of grid points along north direction
n = n1 * n2 # total number of grid points

dn1 = 1/n1
dn2 = 1/n2
sites1 = array((1:n1), c(n1, 1))
sites2 = array((1:n2), c(n2, 1))
ww1 = rep(1, n1)
ww2 = rep(1, n2)
sites1m = sites1 %*% t(ww1) # sites1m is the matrix version of sites1
sites2m = ww2 %*% t(sites2)

sites1v = matrix(sites1m, nrow = n, ncol = 1)
sites2v = matrix(sites2m, nrow = n, ncol = 1)

plot(sites1v, sites2v)


# Compute the distance matrix
ddE = sites1v %*% matrix(rep(1, n), nrow = 1, ncol = n) - matrix(rep(1, n), nrow = n, ncol = 1) %*% t(sites1v)
dd2E = ddE * ddE
ddN = sites2v %*% matrix(rep(1, n), nrow = 1, ncol = n) - matrix(rep(1, n), nrow = n, ncol = 1) %*% t(sites2v)
dd2N = ddN * ddN
t = sqrt(dd2E + dd2N)
image(t, main = "distance matrix")

# Simulate the initial random field
alpha = 1.0 # beta as in regression model
sigma = 1.0  # scaling coef in matern kernel
phi = 10 # range coef in matern kernel
# eta = 10 # range coef in matern kernel
tau = .05 # iid noise

beta1 = -alpha
beta2 = alpha
beta3 = alpha

BETA_TRUE = matrix(c(beta1, beta2, beta3), nrow = 3, ncol = 1)
THETA_TRUE = matrix(c(sigma, phi, tau), nrow = 3, ncol = 1)

Sigma = Matern_cov(sigma, phi, t)  # matern covariance
image(Sigma, main = "matern cov")

L = np.linalg.cholesky(Sigma)  # lower t    riangle matrix
# L = np.linalg.cholesky(C_theta)  # lower triangle matrix

x = np.dot(L, np.random.randn(n).reshape(-1, 1))
H = design_matrix(sites1v, sites2v) # different notation for the project
mu_prior = mu(H, BETA_TRUE).reshape(n, 1)
plotf(np.copy(mu_prior).reshape(n1, n2), "prior mean")
mu_real = mu_prior + x  # add covariance
plotf(np.copy(mu_real).reshape(n1, n2), "realisation of grf")

# sampling from realisations
M = 200
F, ind = sampling_design(n, M)
G = np.dot(F, H)
y_sampled = np.dot(F, mu_real) + tau * np.random.randn(M).reshape(-1, 1)
x_ind, y_ind = np.unravel_index(ind, (n1, n2))

# print(ind)
# print(x_ind)
# print(y_ind)
x_ind = sites1[x_ind]
y_ind = sites2[y_ind]

plt.figure()
plt.scatter(x_ind, y_ind, 100 * abs(y_sampled), facecolors='none', edgecolors='k')
# y_sampled times 10 for scaling in the plot
# abs() added here to make the scatter plot work properly
plt.title("random sample, circle size indicates true mean value")
plt.show()

```

***

## 2.2 Paramter estimation

We will now use the simulated data to estimate the model parameters α,σ2,τ2,φ using maximum likelihood estimation. Iterate between the update for the mean parameter, and updating the covariance parameters. Monitor the likelihood function at each step of the algorithm to check convergence.

```{r, GP simulation}

```


*** 

## 2.3 Kriging

We will now use the estimated model parameters to perform kriging prediction. Predict variables x(s), where predictions sites lie on a regular grid of size 25x25 for the unit square. Visualize the Kriging surface and the prediction standard error. Compare with the true field.

```{r, GP simulation}

```

***

# Part III












