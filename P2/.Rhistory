y_ind
rdist(x_ind, y_ind)
?'rdist'
??'dist'
install.packages("rdist")
library(rdist)
cdist(x_ind, y_ind, metric = "euclidean", p = 2)
tt = cdist(x_ind, y_ind, metric = "euclidean", p = 2)
View(tt)
View(tt)
plotf(tt, "distance matrix")
plot(x_ind, y_ind)
plot(x_ind, y_ind, 'k')
plot(x_ind, y_ind, '.')
ddE = x_ind %*% matrix(rep(1, M), nrow = 1, ncol = M) - matrix(rep(1, M), nrow = M, ncol = 1) %*% t(x_ind)
dd2E = ddE * ddE
ddN = y_ind %*% matrix(rep(1, M), nrow = 1, ncol = M) - matrix(rep(1, M), nrow = M, col = 1) %*% t(y_ind)
dd2N = ddN * ddN
ddN = y_ind %*% matrix(rep(1, M), nrow = 1, ncol = M) - matrix(rep(1, M), nrow = M, col = 1) %*% t(y_ind)
ddN = y_ind %*% matrix(rep(1, M), nrow = 1, ncol = M) - matrix(rep(1, M), nrow = M, ncol = 1) %*% t(y_ind)
dd2N = ddN * ddN
tsample = sqrt(dd2E + dd2N)
tsample == tt
plotf(tsample)
plotf(tsample, "test")
plotf(tt, "t")
rnorm(1)
rnorm(10)
plot(x_ind, y_ind, cex = abs(mu_real_sampled), main = "Random samples in the field, circle size indicates the relative value")
dd2N = ddN * ddN
tsample = sqrt(dd2E + dd2N)
Sigma_sample = Matern_cov(sigma, phi, tsample)
Lsample = t(chol(Sigma_sample))
xsample = Lsample %*% rnorm(M)
Hsample = array(c(rep(1, M), x_ind, y_ind), dim = c(M, 3)) # sampling design matrix
mu_prior_sample = Hsample %*% BETA_TRUE
mu_real_sample = mu_prior_sample + xsample
plot(x_ind, y_ind, cex = abs(y_sampled), main = "Random samples in the field, circle size indicates the relative value")
plot(x_ind, y_ind, cex = abs(mu_real_sampled), main = "Random samples in the field, circle size indicates the relative value")
plot(x_ind, y_ind, cex = abs(mu_real_sample), main = "Random samples in the field, circle size indicates the relative value")
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
View(t)
dim(t)
source('~/.active-rstudio-document', echo=TRUE)
set.seed(2021)
# Setup the grid
n1 = 25 # number of grid points along east direction
n2 = 25 # number of grid points along north direction
n = n1 * n2 # total number of grid points
dn1 = 1/n1
dn2 = 1/n2
sites1 = array(seq(0, 1, dn1), c(n1, 1))
sites2 = array(seq(0, 1, dn2), c(n2, 1))
ww1 = rep(1, n1)
ww2 = rep(1, n2)
sites1m = sites1 %*% t(ww1) # sites1m is the matrix version of sites1
sites2m = ww2 %*% t(sites2)
sites1v = matrix(sites1m, nrow = n, ncol = 1)
sites2v = matrix(sites2m, nrow = n, ncol = 1)
plot(sites1v, sites2v)
# Compute the distance matrix
ddE = sites1v %*% matrix(rep(1, n), nrow = 1, ncol = n) - matrix(rep(1, n), nrow = n, ncol = 1) %*% t(sites1v)
dd2E = ddE * ddE
ddN = sites2v %*% matrix(rep(1, n), nrow = 1, ncol = n) - matrix(rep(1, n), nrow = n, ncol = 1) %*% t(sites2v)
dd2N = ddN * ddN
t = sqrt(dd2E + dd2N)
levelplot(t, col.regions = coul, main = "Distance matrix")
levelplot(t, col.regions = coul, main = "Distance matrix")
# Simulate the initial random field
alpha = 1.0 # beta as in regression model
sigma = 1.0  # scaling coef in matern kernel
phi = 10 # range coef in matern kernel
phi = 10 # range coef in matern kernel
# eta = 10 # range coef in matern kernel
tau = .05 # iid noise
beta1 = -alpha
beta2 = alpha
beta3 = alpha
BETA_TRUE = matrix(c(beta1, beta2, beta3), nrow = 3, ncol = 1)
THETA_TRUE = matrix(c(sigma, phi, tau), nrow = 3, ncol = 1)
Sigma = Matern_cov(sigma, phi, t)  # matern covariance
L = t(chol(Sigma)) # lower L
x = L %*% rnorm(n) # sample from zero mean random variables
H = array(c(rep(1, n), sites1v, sites2v), dim = c(n, 3)) # design matrix
mu_prior = H %*% BETA_TRUE
plotf2d(mu_prior, "Prior mean")
plotf2d(mu_prior, "Prior mean")
mu_real = mu_prior + x
plotf2d(mu_real, "Realisation of the fields")
# sampling from realisations in the regular grids
M = 200
Fmatrix = matrix(0, M, n)
ind = sample(n, size = M, replace = FALSE)
for (i in c(1:M)){
Fmatrix[i, ind[i]] = TRUE
}
G = Fmatrix %*% H
y_sampled = Fmatrix %*% mu_real + tau * rnorm(M, 1)
x_ind = sites1v[ind]
y_ind = sites2v[ind]
plot(x_ind, y_ind, cex = abs(y_sampled), main = "Random samples in the grid, circle size indicates the relative value")
# sampling randomly without grids
while (TRUE){
x_ind = runif(M)
y_ind = runif(M)
if ((unique(x_ind) == x_ind) && (unique(y_ind) == y_ind)){
dim(x_ind) <- c(M, 1)
dim(y_ind) <- c(M, 1)
break;
}
}
ddE = x_ind %*% matrix(rep(1, M), nrow = 1, ncol = M) - matrix(rep(1, M), nrow = M, ncol = 1) %*% t(x_ind)
dd2E = ddE * ddE
ddN = y_ind %*% matrix(rep(1, M), nrow = 1, ncol = M) - matrix(rep(1, M), nrow = M, ncol = 1) %*% t(y_ind)
dd2N = ddN * ddN
tsample = sqrt(dd2E + dd2N)
Sigma_sample = Matern_cov(sigma, phi, tsample)
Lsample = t(chol(Sigma_sample))
xsample = Lsample %*% rnorm(M)
Hsample = array(c(rep(1, M), x_ind, y_ind), dim = c(M, 3)) # sampling design matrix
mu_prior_sample = Hsample %*% BETA_TRUE
mu_real_sample = mu_prior_sample + xsample
plot(x_ind, y_ind, cex = abs(mu_real_sample), main = "Random samples in the square, circle size indicates the relative value")
plot(x_ind, y_ind, cex = abs(mu_real_sample), main = "Random samples in the square, circle size indicates the relative value")
***
## 2.2 Paramter estimation
We will now use the simulated data to estimate the model parameters $\alpha, \sigma^2, \tau^2, \phi$ using maximum likelihood estimation. Iterate between the update for the mean parameter, and updating the covariance parameters. Monitor the likelihood function at each step of the algorithm to check convergence. Since the sampling randomly in the field can cause problems in the distance matrix and accordingly the rest of the calculations. Therefore, it is more stable to use the random samples from the regular grids.
The mean of the field is modelled by $p(\boldsymbol{x})$ and the imperfect information $\boldsymbol{y} = (y_1, \dots, y_m)$ conditional on $\boldsymbol{x}$ can be modelled by $p(\boldsymbol{y}|\boldsymbol{x})$, which can be expressed as follows:
\begin{equation*}
p(\boldsymbol{x}) = \mathop{N}(\boldsymbol{H}\beta, \boldsymbol{\Sigma}), \quad p(\boldsymbol{y}|\boldsymbol{x}) = \mathop{N}(\boldsymbol{Fx}, \boldsymbol{T})
\end{equation*}
Therefore, the marginal likelihood of the data is
\begin{equation*}
p(\boldsymbol{y}) = \mathop{N}(\boldsymbol{G\beta}, \boldsymbol{C}), \quad \boldsymbol{G} = \boldsymbol{FH}, \quad \boldsymbol{C} = \boldsymbol{F\Sigma F^T} + \boldsymbol{T}
\end{equation*}
The log-likelihood as a function of $\beta$ and unknown fixed nuisance parameters $\boldsymbol{\theta}$ in the prior covariance matrix $\boldsymbol{\Sigma} = \boldsymbol{\Sigma(\boldsymbol{\theta})}$, and/or the likelihood noise matrix $\boldsymbol{T} = \boldsymbol{T}(\boldsymbol{\theta})$ becomes
\begin{equation*}
l(\boldsymbol{\theta}, \boldsymbol{\beta}) = -\frac{m}{2} \log(2\pi) - \frac{1}{2}\log|\boldsymbol{C}| - \frac{1}{2}(\boldsymbol{y} - \boldsymbol{G}\boldsymbol{\beta})^T\boldsymbol{C}^{-1}(\boldsymbol{y} - \boldsymbol{G\beta})
\end{equation*}
The MLEs of $\boldsymbol{\beta}$ and $\boldsymbol{\theta}$ are obtained by
\begin{equation*}
(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\theta}}) = \underset{\boldsymbol{\beta}, \boldsymbol{\theta}}{\arg\max}\{l(\boldsymbol{\beta, \theta})\}
\end{equation*}
For fixed $\boldsymbol{\theta}$, the MLEs of $\boldsymbol{\beta}$ can be determined analytically.
\begin{equation*}
\frac{dl}{d\boldsymbol{\beta}} = \boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{y} - \boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{G}\boldsymbol{\beta} = \boldsymbol{0}, \quad \hat{\boldsymbol{\beta}} = (\boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{G})^{-1}\boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{y}
\end{equation*}
Whereas for fixed $\boldsymbol{\beta}$, the MLE of nuisance parameters $\boldsymbol{\theta}$ can be obtained by numerical maximization. Let $\boldsymbol{z} = \boldsymbol{y} - \boldsymbol{G\beta}$, and $\boldsymbol{Q} = \boldsymbol{C}^{-1}$. For each component of $\boldsymbol{\theta}_r$, $r = 1, \dots, d$, in this case, $\theta$ has 3 components ($\sigma^2, \eta, \tau^2$). The score of the log-likelihood becomes
\begin{equation*}
\frac{dl}{d\theta_r} = -\frac{1}{2}\text{trace}(\boldsymbol{Q}\frac{d\boldsymbol{C}}{d\theta_r}) + \frac{1}{2}\boldsymbol{z}^T\boldsymbol{Q}\frac{d\boldsymbol{C}}{d\theta_r}\boldsymbol{Qz}
\end{equation*}
The above mentioned score can be solved iteratively using Fisher scoring algorithm. To achieve the numerical stability, the expected Hessian is applied, which is
\begin{equation*}
\mathop{E}(\frac{d^2l}{d\theta_rd\theta_{\bar{r}}}) = -\frac{1}{2}\text{trace}(\boldsymbol{Q}\frac{d\boldsymbol{C}}{d\theta_{\bar{r}}}\boldsymbol{Q}\frac{d\boldsymbol{C}}{d\theta_r})
\end{equation*}
The pseudo code for the Fisher scoring algorithm can then be  expressed as follows:
\begin{algorithm}[H]
\KwData{initial $\beta_0$, $\theta_0$}
\KwResult{Converged $\hat{\beta}$, $\hat{\theta}$}
\While{not converged}{
$\boldsymbol{C} = \boldsymbol{C}(\boldsymbol{\theta}^b)$\;
$\boldsymbol{\beta}^{b+1} = [\boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{G}]^{-1}\boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{y}$\;
$\boldsymbol{z} = \boldsymbol{y} - \boldsymbol{G}\boldsymbol{\beta}^{b + 1}$\;
$\boldsymbol{Q} = \boldsymbol{C}^{-1}$ \;
$\boldsymbol{C}^*_r = \frac{d\boldsymbol{C}(\boldsymbol{\theta}^b)}{d\theta_r}, \quad r = 1, \dots, d$ \;
$u_r = \frac{dl}{d\theta_r} = -\frac{1}{2}\text{trace}(\boldsymbol{Q}\boldsymbol{C}_r^*) + \frac{1}{2}\boldsymbol{z}^T\boldsymbol{Q}\boldsymbol{C}_r^*\boldsymbol{Qz}$\;
$\boldsymbol{V}_{r\bar{r}}=\mathop{E}(\frac{d^2l}{d\theta_rd\theta_{\bar{r}}}) = -\frac{1}{2}\text{trace}(\boldsymbol{Q}\boldsymbol{C}_r^*\boldsymbol{Q}\boldsymbol{C}_{\bar{r}}^*)$\;
$\boldsymbol{\theta}^{b + 1} = \boldsymbol{\theta}^b + \boldsymbol{V}^{-1}\boldsymbol{u}$\;
$b = b + 1$
}
\end{algorithm}
```{r, parameter estimation}
#%%
C_matrix <- function(theta){
sigma = theta[1]
phi = theta[2]
tau = theta[3]
Sigma = Matern_cov(sigma, phi, t)
C = Fmatrix %*% Sigma %*% t(Fmatrix) + diag(M) * tau^2
return(C)
}
dC_dsigma <- function(theta){
# sigma = theta$sigma
# phi = theta$phi
# tau = theta$tau
sigma = theta[1]
phi = theta[2]
tau = theta[3]
Km = Matern_cov(1.0, phi, t) # t here is the distance matrix, H is the design matrix, similar to X
dC_dsgm = Fmatrix %*% Km %*% t(Fmatrix)
return(dC_dsgm)
}
dC_dphi <- function(theta){
sigma = theta[1]
phi = theta[2]
tau = theta[3]
Kn = sigma ^ 2 * (-phi * t) * exp(-phi * t)
return(Fmatrix %*% Kn %*% t(Fmatrix))
}
dC_dtau <- function(theta){
return(diag(M))
}
# Use fisher scoring to find MLE parameters
# beta = np.zeros([3, 1])
beta = matrix(c(-.9, 1.1, 1.2), nrow = 3, ncol = 1)
theta = rbind(0.9, 9.5, .03)
MAX_ITER = 500
No_iter = 0
epsilon = 10
Beta = matrix(0, nrow = MAX_ITER, ncol = 3)
Likelihood = matrix(0, nrow = MAX_ITER, ncol = 1)
while (No_iter < MAX_ITER & epsilon > .0001){
C = C_matrix(theta)
beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
Beta[No_iter, ] = t(beta)
z = y_sampled - G %*% beta
lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
Likelihood[No_iter, ] = lik
# Find dC*/dtheta
dC_dSgm = dC_dsigma(theta)
dC_dPhi = dC_dphi(theta)
dC_dTau = dC_dtau(theta)
u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm))) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z))
u_eta = -1 / 2 * sum(diag(solve(C, dC_dPhi))) + 1 / 2 * t(z) %*% solve(C, dC_dPhi %*% solve(C, z))
u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau))) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z))
u = rbind(u_sigma, u_eta, u_tau)
V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
theta = theta_new
print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
# Use fisher scoring to find MLE parameters
# beta = np.zeros([3, 1])
beta = matrix(c(-1.2, 1.1, 1.2), nrow = 3, ncol = 1)
theta = rbind(1.2, 8.5, .04)
MAX_ITER = 500
No_iter = 0
epsilon = 10
Beta = matrix(0, nrow = MAX_ITER, ncol = 3)
Likelihood = matrix(0, nrow = MAX_ITER, ncol = 1)
while (No_iter < MAX_ITER & epsilon > .0001){
C = C_matrix(theta)
beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
Beta[No_iter, ] = t(beta)
z = y_sampled - G %*% beta
lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
Likelihood[No_iter, ] = lik
# Find dC*/dtheta
dC_dSgm = dC_dsigma(theta)
dC_dPhi = dC_dphi(theta)
dC_dTau = dC_dtau(theta)
u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm))) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z))
u_eta = -1 / 2 * sum(diag(solve(C, dC_dPhi))) + 1 / 2 * t(z) %*% solve(C, dC_dPhi %*% solve(C, z))
u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau))) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z))
u = rbind(u_sigma, u_eta, u_tau)
V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
theta = theta_new
print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
set.seed(2021)
# Setup the grid
n1 = 25 # number of grid points along east direction
n2 = 25 # number of grid points along north direction
n = n1 * n2 # total number of grid points
dn1 = 1/n1
dn2 = 1/n2
sites1 = array(seq(0, 1, dn1), c(n1, 1))
sites2 = array(seq(0, 1, dn2), c(n2, 1))
ww1 = rep(1, n1)
ww2 = rep(1, n2)
sites1m = sites1 %*% t(ww1) # sites1m is the matrix version of sites1
sites2m = ww2 %*% t(sites2)
sites1v = matrix(sites1m, nrow = n, ncol = 1)
sites2v = matrix(sites2m, nrow = n, ncol = 1)
plot(sites1v, sites2v)
# Compute the distance matrix
ddE = sites1v %*% matrix(rep(1, n), nrow = 1, ncol = n) - matrix(rep(1, n), nrow = n, ncol = 1) %*% t(sites1v)
ddN = sites2v %*% matrix(rep(1, n), nrow = 1, ncol = n) - matrix(rep(1, n), nrow = n, ncol = 1) %*% t(sites2v)
dd2N = ddN * ddN
t = sqrt(dd2E + dd2N)
levelplot(t, col.regions = coul, main = "Distance matrix")
levelplot(t, col.regions = coul, main = "Distance matrix")
# Simulate the initial random field
alpha = 1.0 # beta as in regression model
sigma = 1.0  # scaling coef in matern kernel
phi = 10 # range coef in matern kernel
phi = 10 # range coef in matern kernel
# eta = 10 # range coef in matern kernel
tau = .05 # iid noise
dd2E = ddE * ddE
ddN = sites2v %*% matrix(rep(1, n), nrow = 1, ncol = n) - matrix(rep(1, n), nrow = n, ncol = 1) %*% t(sites2v)
dd2N = ddN * ddN
t = sqrt(dd2E + dd2N)
levelplot(t, col.regions = coul, main = "Distance matrix")
levelplot(t, col.regions = coul, main = "Distance matrix")
# Simulate the initial random field
alpha = 1.0 # beta as in regression model
sigma = 1.0  # scaling coef in matern kernel
sigma = 1.0  # scaling coef in matern kernel
phi = 10 # range coef in matern kernel
# eta = 10 # range coef in matern kernel
tau = .05 # iid noise
beta1 = -alpha
beta2 = alpha
beta3 = alpha
BETA_TRUE = matrix(c(beta1, beta2, beta3), nrow = 3, ncol = 1)
THETA_TRUE = matrix(c(sigma, phi, tau), nrow = 3, ncol = 1)
Sigma = Matern_cov(sigma, phi, t)  # matern covariance
L = t(chol(Sigma)) # lower L
beta3 = alpha
BETA_TRUE = matrix(c(beta1, beta2, beta3), nrow = 3, ncol = 1)
THETA_TRUE = matrix(c(sigma, phi, tau), nrow = 3, ncol = 1)
Sigma = Matern_cov(sigma, phi, t)  # matern covariance
L = t(chol(Sigma)) # lower L
x = L %*% rnorm(n) # sample from zero mean random variables
# eta = 10 # range coef in matern kernel
tau = .05 # iid noise
beta1 = -alpha
beta2 = alpha
beta3 = alpha
BETA_TRUE = matrix(c(beta1, beta2, beta3), nrow = 3, ncol = 1)
THETA_TRUE = matrix(c(sigma, phi, tau), nrow = 3, ncol = 1)
Sigma = Matern_cov(sigma, phi, t)  # matern covariance
L = t(chol(Sigma)) # lower L
x = L %*% rnorm(n) # sample from zero mean random variables
H = array(c(rep(1, n), sites1v, sites2v), dim = c(n, 3)) # design matrix
plotf2d(mu_prior, "Prior mean")
mu_real = mu_prior + x
plotf2d(mu_real, "Realisation of the fields")
# sampling from realisations in the regular grids
M = 200
Fmatrix = matrix(0, M, n)
ind = sample(n, size = M, replace = FALSE)
for (i in c(1:M)){
Fmatrix[i, ind[i]] = TRUE
}
G = Fmatrix %*% H
y_sampled = Fmatrix %*% mu_real + tau * rnorm(M, 1)
x_ind = sites1v[ind]
y_ind = sites2v[ind]
plot(x_ind, y_ind, cex = abs(y_sampled), main = "Random samples in the grid, circle size indicates the relative value")
# sampling randomly without grids
while (TRUE){
x_ind = runif(M)
y_ind = runif(M)
if ((unique(x_ind) == x_ind) && (unique(y_ind) == y_ind)){
dim(x_ind) <- c(M, 1)
dim(y_ind) <- c(M, 1)
break;
}
}
ddE = x_ind %*% matrix(rep(1, M), nrow = 1, ncol = M) - matrix(rep(1, M), nrow = M, ncol = 1) %*% t(x_ind)
dd2E = ddE * ddE
ddN = y_ind %*% matrix(rep(1, M), nrow = 1, ncol = M) - matrix(rep(1, M), nrow = M, ncol = 1) %*% t(y_ind)
dd2N = ddN * ddN
tsample = sqrt(dd2E + dd2N)
Sigma_sample = Matern_cov(sigma, phi, tsample)
Lsample = t(chol(Sigma_sample))
xsample = Lsample %*% rnorm(M)
Hsample = array(c(rep(1, M), x_ind, y_ind), dim = c(M, 3)) # sampling design matrix
mu_prior_sample = Hsample %*% BETA_TRUE
mu_real_sample = mu_prior_sample + xsample
plot(x_ind, y_ind, cex = abs(mu_real_sample), main = "Random samples in the square, circle size indicates the relative value")
y_sampled = mu_real_sample + tau * rnorm(M, 1)
plot(x_ind, y_ind, cex = abs(mu_real_sample), main = "Random samples in the square, circle size indicates the relative value")
y_sampled = mu_real_sample + tau * rnorm(M, 1)
***
## 2.2 Paramter estimation
We will now use the simulated data to estimate the model parameters $\alpha, \sigma^2, \tau^2, \phi$ using maximum likelihood estimation. Iterate between the update for the mean parameter, and updating the covariance parameters. Monitor the likelihood function at each step of the algorithm to check convergence. Since the sampling randomly in the field can cause problems in the distance matrix and accordingly the rest of the calculations. Therefore, it is more stable to use the random samples from the regular grids.
The mean of the field is modelled by $p(\boldsymbol{x})$ and the imperfect information $\boldsymbol{y} = (y_1, \dots, y_m)$ conditional on $\boldsymbol{x}$ can be modelled by $p(\boldsymbol{y}|\boldsymbol{x})$, which can be expressed as follows:
\begin{equation*}
p(\boldsymbol{x}) = \mathop{N}(\boldsymbol{H}\beta, \boldsymbol{\Sigma}), \quad p(\boldsymbol{y}|\boldsymbol{x}) = \mathop{N}(\boldsymbol{Fx}, \boldsymbol{T})
\end{equation*}
Therefore, the marginal likelihood of the data is
\begin{equation*}
p(\boldsymbol{y}) = \mathop{N}(\boldsymbol{G\beta}, \boldsymbol{C}), \quad \boldsymbol{G} = \boldsymbol{FH}, \quad \boldsymbol{C} = \boldsymbol{F\Sigma F^T} + \boldsymbol{T}
\end{equation*}
The log-likelihood as a function of $\beta$ and unknown fixed nuisance parameters $\boldsymbol{\theta}$ in the prior covariance matrix $\boldsymbol{\Sigma} = \boldsymbol{\Sigma(\boldsymbol{\theta})}$, and/or the likelihood noise matrix $\boldsymbol{T} = \boldsymbol{T}(\boldsymbol{\theta})$ becomes
\begin{equation*}
l(\boldsymbol{\theta}, \boldsymbol{\beta}) = -\frac{m}{2} \log(2\pi) - \frac{1}{2}\log|\boldsymbol{C}| - \frac{1}{2}(\boldsymbol{y} - \boldsymbol{G}\boldsymbol{\beta})^T\boldsymbol{C}^{-1}(\boldsymbol{y} - \boldsymbol{G\beta})
\end{equation*}
The MLEs of $\boldsymbol{\beta}$ and $\boldsymbol{\theta}$ are obtained by
\begin{equation*}
(\hat{\boldsymbol{\beta}}, \hat{\boldsymbol{\theta}}) = \underset{\boldsymbol{\beta}, \boldsymbol{\theta}}{\arg\max}\{l(\boldsymbol{\beta, \theta})\}
\end{equation*}
For fixed $\boldsymbol{\theta}$, the MLEs of $\boldsymbol{\beta}$ can be determined analytically.
\begin{equation*}
\frac{dl}{d\boldsymbol{\beta}} = \boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{y} - \boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{G}\boldsymbol{\beta} = \boldsymbol{0}, \quad \hat{\boldsymbol{\beta}} = (\boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{G})^{-1}\boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{y}
\end{equation*}
Whereas for fixed $\boldsymbol{\beta}$, the MLE of nuisance parameters $\boldsymbol{\theta}$ can be obtained by numerical maximization. Let $\boldsymbol{z} = \boldsymbol{y} - \boldsymbol{G\beta}$, and $\boldsymbol{Q} = \boldsymbol{C}^{-1}$. For each component of $\boldsymbol{\theta}_r$, $r = 1, \dots, d$, in this case, $\theta$ has 3 components ($\sigma^2, \eta, \tau^2$). The score of the log-likelihood becomes
\begin{equation*}
\frac{dl}{d\theta_r} = -\frac{1}{2}\text{trace}(\boldsymbol{Q}\frac{d\boldsymbol{C}}{d\theta_r}) + \frac{1}{2}\boldsymbol{z}^T\boldsymbol{Q}\frac{d\boldsymbol{C}}{d\theta_r}\boldsymbol{Qz}
\end{equation*}
The above mentioned score can be solved iteratively using Fisher scoring algorithm. To achieve the numerical stability, the expected Hessian is applied, which is
\begin{equation*}
\mathop{E}(\frac{d^2l}{d\theta_rd\theta_{\bar{r}}}) = -\frac{1}{2}\text{trace}(\boldsymbol{Q}\frac{d\boldsymbol{C}}{d\theta_{\bar{r}}}\boldsymbol{Q}\frac{d\boldsymbol{C}}{d\theta_r})
\end{equation*}
The pseudo code for the Fisher scoring algorithm can then be  expressed as follows:
\begin{algorithm}[H]
\KwData{initial $\beta_0$, $\theta_0$}
\KwResult{Converged $\hat{\beta}$, $\hat{\theta}$}
\While{not converged}{
$\boldsymbol{C} = \boldsymbol{C}(\boldsymbol{\theta}^b)$\;
$\boldsymbol{\beta}^{b+1} = [\boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{G}]^{-1}\boldsymbol{G}^T\boldsymbol{C}^{-1}\boldsymbol{y}$\;
$\boldsymbol{z} = \boldsymbol{y} - \boldsymbol{G}\boldsymbol{\beta}^{b + 1}$\;
$\boldsymbol{Q} = \boldsymbol{C}^{-1}$ \;
$\boldsymbol{C}^*_r = \frac{d\boldsymbol{C}(\boldsymbol{\theta}^b)}{d\theta_r}, \quad r = 1, \dots, d$ \;
$u_r = \frac{dl}{d\theta_r} = -\frac{1}{2}\text{trace}(\boldsymbol{Q}\boldsymbol{C}_r^*) + \frac{1}{2}\boldsymbol{z}^T\boldsymbol{Q}\boldsymbol{C}_r^*\boldsymbol{Qz}$\;
$\boldsymbol{V}_{r\bar{r}}=\mathop{E}(\frac{d^2l}{d\theta_rd\theta_{\bar{r}}}) = -\frac{1}{2}\text{trace}(\boldsymbol{Q}\boldsymbol{C}_r^*\boldsymbol{Q}\boldsymbol{C}_{\bar{r}}^*)$\;
$\boldsymbol{\theta}^{b + 1} = \boldsymbol{\theta}^b + \boldsymbol{V}^{-1}\boldsymbol{u}$\;
$b = b + 1$
}
\end{algorithm}
```{r, parameter estimation}
#%%
C_matrix <- function(theta){
sigma = theta[1]
phi = theta[2]
tau = theta[3]
Sigma = Matern_cov(sigma, phi, t)
C = Fmatrix %*% Sigma %*% t(Fmatrix) + diag(M) * tau^2
return(C)
}
dC_dsigma <- function(theta){
# sigma = theta$sigma
# phi = theta$phi
# tau = theta$tau
sigma = theta[1]
phi = theta[2]
tau = theta[3]
Km = Matern_cov(1.0, phi, t) # t here is the distance matrix, H is the design matrix, similar to X
dC_dsgm = Fmatrix %*% Km %*% t(Fmatrix)
return(dC_dsgm)
}
dC_dphi <- function(theta){
sigma = theta[1]
phi = theta[2]
tau = theta[3]
Kn = sigma ^ 2 * (-phi * t) * exp(-phi * t)
return(Fmatrix %*% Kn %*% t(Fmatrix))
}
dC_dtau <- function(theta){
return(diag(M))
}
# Use fisher scoring to find MLE parameters
beta = matrix(c(-1.1, 0.9, 0.9), nrow = 3, ncol = 1)
theta = rbind(1.2, 8.5, .04)
MAX_ITER = 500
No_iter = 0
epsilon = 10
Beta = matrix(0, nrow = MAX_ITER, ncol = 3)
Likelihood = matrix(0, nrow = MAX_ITER, ncol = 1)
while (No_iter < MAX_ITER & epsilon > .0001){
C = C_matrix(theta)
beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
Beta[No_iter, ] = t(beta)
z = y_sampled - G %*% beta
lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
Likelihood[No_iter, ] = lik
# Find dC*/dtheta
dC_dSgm = dC_dsigma(theta)
dC_dPhi = dC_dphi(theta)
dC_dTau = dC_dtau(theta)
u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm))) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z))
u_eta = -1 / 2 * sum(diag(solve(C, dC_dPhi))) + 1 / 2 * t(z) %*% solve(C, dC_dPhi %*% solve(C, z))
u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau))) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z))
u = rbind(u_sigma, u_eta, u_tau)
V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
theta = theta_new
print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
