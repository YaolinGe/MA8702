}
dC_dsigma <- function(theta){
# sigma = theta$sigma
# phi = theta$phi
# tau = theta$tau
sigma = theta[1]
phi = theta[2]
tau = theta[3]
Km = Matern_cov(1.0, phi, t) # t here is the distance matrix, H is the design matrix, similar to X
dC_dsgm = Fmatrix %*% Km %*% t(Fmatrix)
return(dC_dsgm)
}
dC_dphi <- function(theta){
# sigma = theta$sigma
# phi = theta$phi
# tau = theta$tau
sigma = theta[1]
phi = theta[2]
tau = theta[3]
Kn = sigma ^ 2 * (-phi * t) * exp(-phi * t)
return(Fmatrix %*% Kn %*% t(Fmatrix))
}
dC_dtau <- function(theta){
return(diag(M))
}
# Use fisher scoring to find MLE parameters
# beta = np.zeros([3, 1])
beta = matrix(c(-2.1, 3.1, .9), nrow = 3, ncol = 1)
theta = rbind(.245, 9.3, .003)
MAX_ITER = 5000
No_iter = 0
epsilon = 10
Beta = matrix(0, nrow = MAX_ITER, ncol = 3)
Likelihood = matrix(0, nrow = MAX_ITER, ncol = 1)
while (No_iter < MAX_ITER & epsilon > .0001){
sigma = theta[1]
phi = theta[2]
tau = theta[3]
print(sigma)
print(phi)
print(tau)
C = C_matrix(theta)
# beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
# # beta = np.linalg.solve(np.dot(G.T, np.linalg.solve(C, G)), np.dot(G.T, np.linalg.solve(C, y_sampled)))
# Beta[No_iter, ] = t(beta)
# z = y_sampled - G %*% beta
# lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
# Likelihood[No_iter, ] = lik
# Find dC*/dtheta
# dC_dSgm = dC_dsigma(theta)
# dC_dPhi = dC_dphi(theta)
# dC_dTau = dC_dtau(theta)
#
# u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm)) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z)))
# u_eta = -1 / 2 * sum(diag(solve(C, dC_dEta)) + 1 / 2 * t(z) %*% solve(C, dC_dEta %*% solve(C, z)))
# u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau)) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z)))
#
# u = rbind(u_sigma, u_eta, u_tau)
#
# V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
# V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
# V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
# V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
# V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
# V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
# V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
# V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
# V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
#
# V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
# print(lik)
# theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
# epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
# theta = theta_new
# print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
MAX_ITER = 5
No_iter = 0
epsilon = 10
Beta = matrix(0, nrow = MAX_ITER, ncol = 3)
Likelihood = matrix(0, nrow = MAX_ITER, ncol = 1)
while (No_iter < MAX_ITER & epsilon > .0001){
sigma = theta[1]
phi = theta[2]
tau = theta[3]
print(sigma)
print(phi)
print(tau)
C = C_matrix(theta)
# beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
# # beta = np.linalg.solve(np.dot(G.T, np.linalg.solve(C, G)), np.dot(G.T, np.linalg.solve(C, y_sampled)))
# Beta[No_iter, ] = t(beta)
# z = y_sampled - G %*% beta
# lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
# Likelihood[No_iter, ] = lik
# Find dC*/dtheta
# dC_dSgm = dC_dsigma(theta)
# dC_dPhi = dC_dphi(theta)
# dC_dTau = dC_dtau(theta)
#
# u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm)) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z)))
# u_eta = -1 / 2 * sum(diag(solve(C, dC_dEta)) + 1 / 2 * t(z) %*% solve(C, dC_dEta %*% solve(C, z)))
# u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau)) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z)))
#
# u = rbind(u_sigma, u_eta, u_tau)
#
# V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
# V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
# V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
# V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
# V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
# V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
# V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
# V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
# V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
#
# V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
# print(lik)
# theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
# epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
# theta = theta_new
# print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
while (No_iter < MAX_ITER & epsilon > .0001){
C = C_matrix(theta)
# beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
# # beta = np.linalg.solve(np.dot(G.T, np.linalg.solve(C, G)), np.dot(G.T, np.linalg.solve(C, y_sampled)))
# Beta[No_iter, ] = t(beta)
# z = y_sampled - G %*% beta
# lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
# Likelihood[No_iter, ] = lik
# Find dC*/dtheta
# dC_dSgm = dC_dsigma(theta)
# dC_dPhi = dC_dphi(theta)
# dC_dTau = dC_dtau(theta)
#
# u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm)) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z)))
# u_eta = -1 / 2 * sum(diag(solve(C, dC_dEta)) + 1 / 2 * t(z) %*% solve(C, dC_dEta %*% solve(C, z)))
# u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau)) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z)))
#
# u = rbind(u_sigma, u_eta, u_tau)
#
# V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
# V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
# V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
# V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
# V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
# V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
# V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
# V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
# V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
#
# V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
# print(lik)
# theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
# epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
# theta = theta_new
# print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
while (No_iter < MAX_ITER & epsilon > .0001){
C = C_matrix(theta)
beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
# beta = np.linalg.solve(np.dot(G.T, np.linalg.solve(C, G)), np.dot(G.T, np.linalg.solve(C, y_sampled)))
Beta[No_iter, ] = t(beta)
z = y_sampled - G %*% beta
lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
Likelihood[No_iter, ] = lik
# Find dC*/dtheta
# dC_dSgm = dC_dsigma(theta)
# dC_dPhi = dC_dphi(theta)
# dC_dTau = dC_dtau(theta)
#
# u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm)) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z)))
# u_eta = -1 / 2 * sum(diag(solve(C, dC_dEta)) + 1 / 2 * t(z) %*% solve(C, dC_dEta %*% solve(C, z)))
# u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau)) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z)))
#
# u = rbind(u_sigma, u_eta, u_tau)
#
# V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
# V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
# V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
# V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
# V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
# V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
# V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
# V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
# V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
#
# V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
print(lik)
# theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
# epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
# theta = theta_new
# print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
# V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
# V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
# V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
# V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
# V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
# V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
# V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
# V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
#
# V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
print(lik)
while (No_iter < MAX_ITER & epsilon > .0001){
C = C_matrix(theta)
beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
# beta = np.linalg.solve(np.dot(G.T, np.linalg.solve(C, G)), np.dot(G.T, np.linalg.solve(C, y_sampled)))
Beta[No_iter, ] = t(beta)
z = y_sampled - G %*% beta
lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
Likelihood[No_iter, ] = lik
# Find dC*/dtheta
dC_dSgm = dC_dsigma(theta)
dC_dPhi = dC_dphi(theta)
dC_dTau = dC_dtau(theta)
#
# u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm)) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z)))
# u_eta = -1 / 2 * sum(diag(solve(C, dC_dEta)) + 1 / 2 * t(z) %*% solve(C, dC_dEta %*% solve(C, z)))
# u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau)) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z)))
#
# u = rbind(u_sigma, u_eta, u_tau)
#
# V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
# V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
# V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
# V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
# V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
# V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
# V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
# V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
# V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
#
# V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
print(lik)
# theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
# epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
# theta = theta_new
# print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
while (No_iter < MAX_ITER & epsilon > .0001){
C = C_matrix(theta)
beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
# beta = np.linalg.solve(np.dot(G.T, np.linalg.solve(C, G)), np.dot(G.T, np.linalg.solve(C, y_sampled)))
Beta[No_iter, ] = t(beta)
z = y_sampled - G %*% beta
lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
Likelihood[No_iter, ] = lik
# Find dC*/dtheta
dC_dSgm = dC_dsigma(theta)
dC_dPhi = dC_dphi(theta)
dC_dTau = dC_dtau(theta)
u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm)) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z)))
u_eta = -1 / 2 * sum(diag(solve(C, dC_dEta)) + 1 / 2 * t(z) %*% solve(C, dC_dEta %*% solve(C, z)))
u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau)) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z)))
u = rbind(u_sigma, u_eta, u_tau)
V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
# print(lik)
print(No_iter)
# theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
# epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
# theta = theta_new
# print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
while (No_iter < MAX_ITER & epsilon > .0001){
C = C_matrix(theta)
beta = solve(t(G) %*% solve(C, G), t(G) %*% solve(C, y_sampled))
# beta = np.linalg.solve(np.dot(G.T, np.linalg.solve(C, G)), np.dot(G.T, np.linalg.solve(C, y_sampled)))
Beta[No_iter, ] = t(beta)
z = y_sampled - G %*% beta
lik = -M/2 * log(2 * pi) - 1/2 * log(det(C)) -  1/2 * t(z) %*% solve(C, z) # otherwise, it becomes inf
Likelihood[No_iter, ] = lik
# Find dC*/dtheta
dC_dSgm = dC_dsigma(theta)
dC_dPhi = dC_dphi(theta)
dC_dTau = dC_dtau(theta)
u_sigma = -1/2 * sum(diag(solve(C, dC_dSgm)) + 1/2 * t(z) %*% solve(C, dC_dSgm %*% solve(C, z)))
u_eta = -1 / 2 * sum(diag(solve(C, dC_dEta)) + 1 / 2 * t(z) %*% solve(C, dC_dEta %*% solve(C, z)))
u_tau = -1 / 2 * sum(diag(solve(C, dC_dTau)) + 1 / 2 * t(z) %*% solve(C, dC_dTau %*% solve(C, z)))
u = rbind(u_sigma, u_eta, u_tau)
V11 = -1/2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dSgm)))))
V12 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dPhi)))))
V13 = -1 / 2 * sum(diag(solve(C, (dC_dSgm %*% solve(C, dC_dTau)))))
V21 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dSgm)))))
V22 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dPhi)))))
V23 = -1 / 2 * sum(diag(solve(C, (dC_dPhi %*% solve(C, dC_dTau)))))
V31 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dSgm)))))
V32 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dPhi)))))
V33 = -1 / 2 * sum(diag(solve(C, (dC_dTau %*% solve(C, dC_dTau)))))
V = matrix(c(V11, V12, V13, V21, V22, V23, V31, V32, V33), nrow = 3, ncol = 3)
# print(lik)
# print(No_iter)
theta_new = theta - solve(V, u)  # here it is minus, but in the book, it says plus, needs to be rechecked
epsilon = norm(theta_new - theta, type = "2") / norm(beta, type = "2")
theta = theta_new
print(paste(epsilon , " , iter no is ", No_iter))
No_iter = No_iter + 1
}
plot(Likelihood, 'k')
Likelihood
plot(Likelihood[, 1], 'k')
plot(c(1:No_iter), Likelihood[], 'k')
plot(c(1:No_iter), Likelihood, 'k')
plot(c(1:No_iter), Likelihood)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
# print('Estimated sigma is ', sigmah, "\nEstimated eta is ", etah, \
#       "\nEstimated tau is ", tauh, "\nEstimated alpha is ", alphah)
print(cat("\nEstimated sigma is ", round(sigmah, digits = 3), "; True sigma is ", THETA_TRUE[0],
"\nEstimated eta is ", round(etah, digits = 2), "; True eta is ", THETA_TRUE[1],
"\nEstimated tau is ", round(tauh, digits = 5), "; True tau is ", THETA_TRUE[2],
"\nEstimated beta1 is ", round(beta1, digits = 2), "; True beta1 is ", BETA_TRUE[0],
"\nEstimated beta2 is ", round(beta2, digits = 2), "; True beta2 is ", BETA_TRUE[1],
"\nEstimated beta3 is ", round(beta3, digits = 2), "; True beta3 is ", BETA_TRUE[2]))
# print('Estimated sigma is ', sigmah, "\nEstimated eta is ", etah, \
#       "\nEstimated tau is ", tauh, "\nEstimated alpha is ", alphah)
print(cat("\nEstimated sigma is ", round(sigmah, digits = 3), "; True sigma is ", THETA_TRUE[1],
"\nEstimated eta is ", round(etah, digits = 2), "; True eta is ", THETA_TRUE[2],
"\nEstimated tau is ", round(tauh, digits = 5), "; True tau is ", THETA_TRUE[3],
"\nEstimated beta1 is ", round(beta1, digits = 2), "; True beta1 is ", BETA_TRUE[1],
"\nEstimated beta2 is ", round(beta2, digits = 2), "; True beta2 is ", BETA_TRUE[2],
"\nEstimated beta3 is ", round(beta3, digits = 2), "; True beta3 is ", BETA_TRUE[3]))
Sigmah = Matern_cov(sigmah, phih, t) # estimated covariance matrix
# alphah = (sum(np.abs(beta)) / 3).squeeze()
thetah = theta
sigmah = theta[1]
phih = theta[2]
tauh = theta[3]
betah = beta
beta1 = beta[1]
beta2 = beta[2]
beta3 = beta[3]
# print('Estimated sigma is ', sigmah, "\nEstimated eta is ", etah, \
#       "\nEstimated tau is ", tauh, "\nEstimated alpha is ", alphah)
print(cat("\nEstimated sigma is ", round(sigmah, digits = 3), "; True sigma is ", THETA_TRUE[1],
"\nEstimated phi is ", round(phih, digits = 2), "; True phi is ", THETA_TRUE[2],
"\nEstimated tau is ", round(tauh, digits = 5), "; True tau is ", THETA_TRUE[3],
"\nEstimated beta1 is ", round(beta1, digits = 2), "; True beta1 is ", BETA_TRUE[1],
"\nEstimated beta2 is ", round(beta2, digits = 2), "; True beta2 is ", BETA_TRUE[2],
"\nEstimated beta3 is ", round(beta3, digits = 2), "; True beta3 is ", BETA_TRUE[3]))
Sigmah = Matern_cov(sigmah, phih, t) # estimated covariance matrix
Lh = chol(Sigmah)
mh = H %*% betah + Lh %*% rnorm(n)
xp = mh + Sigmah %*% t(Fmatrix) %*% solve(C, (y_sampled - Fmatrix %*% mh))
plotf(xp, "posterior mean")
Sigmap = Sigmah - Sigmah %*% t(Fmatrix) %*% solve(Ch, Fmatrix %*% Sigmah)
Ch = C_matrix(thetah)
xp = mh + Sigmah %*% t(Fmatrix) %*% solve(C, (y_sampled - Fmatrix %*% mh))
plotf(xp, "posterior mean")
Sigmap = Sigmah - Sigmah %*% t(Fmatrix) %*% solve(Ch, Fmatrix %*% Sigmah)
plotf(estd, "posterior std")
# plotf(Sigmap, "posterior covariance")
estd = sqrt(diag(Sigmap))
plotf(estd, "posterior std")
MSE = sqrt(sum(abs(xp - mu_real) ** 2) / n)
print("The prediction error is ", MSE)
MSE = sqrt(sum(abs(xp - mu_real) ** 2) / n)
print("The prediction error is ", MSE)
print(cat("The prediction error is ", MSE))
print(cat("The prediction error is ", MSE))
MSE
print(paste("The prediction error is ", MSE))
a = rnorm(4, 4, )
a
a = rnorm(10, 4, 4)
a
suppressPackageStartupMessages(library(knitr))
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE, error = FALSE)
# Plot the covariance matrix
levelplot(t(Sigma),
col.regions = coul, main = "Covariance matrix",
ylim=c(100,1))
source('~/.active-rstudio-document', echo=TRUE)
# sizes
n <- 100
# define regular grid of locations
sites1v <- array((1:n),c(n,1))
# Prior mean
m <- 0
# compute East and North distances on grid
ww <- array(1,c(n,1))
# determine the distance matrix
H <- abs(sites1v%*%t(ww)-ww %*% t(sites1v))
# Exponential covariance model
Sigma <- exp(-0.1*H)
# Plot the covariance matrix
levelplot(t(Sigma),
col.regions = coul, main = "Covariance matrix",
ylim=c(100,1))
# Plot the covariance matrix
levelplot(t(Sigma),
col.regions = coul, main = "Covariance matrix",
ylim=c(100,1))
install.packages("INLA",repos=c(getOption("repos"),INLA="https://inla.r-inla-download.org/R/stable"), dep=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
suppressPackageStartupMessages(library(knitr))
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE, error = FALSE)
# define regular grid of locations
sites1v <- array((1:n),c(n,1))
# Prior mean
m <- 0
# compute East and North distances on grid
ww <- array(1,c(n,1))
# determine the distance matrix
H <- abs(sites1v%*%t(ww)-ww %*% t(sites1v))
# Exponential covariance model
Sigma <- exp(-0.1*H)
plotf <- function(val, string){
levelplot(t(val), ylim=c(100,1), col.regions = coul, main = string)
}
# Plot the covariance matrix
plotf(Sigma, "Covariance matrix")
# set.seed(87012021)
# sizes
n <- 100
# define regular grid of locations
sites1v <- array((1:n),c(n,1))
# Prior mean
m <- 0
# compute East and North distances on grid
ww <- array(1,c(n,1))
# determine the distance matrix
H <- abs(sites1v%*%t(ww)-ww %*% t(sites1v))
# Exponential covariance model
Sigma <- exp(-0.1*H)
plotf <- function(val, string){
levelplot(t(val), ylim=c(100,1), col.regions = coul, main = string)
}
# Plot the covariance matrix
plotf(Sigma, "Covariance matrix")
```{r setup, include=TRUE,echo=FALSE}
suppressPackageStartupMessages(library(knitr))
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE, error = FALSE)
suppressPackageStartupMessages(library(knitr))
knitr::opts_chunk$set(echo = FALSE, message=FALSE,warning = FALSE, error = FALSE)
```{r load libraries, echo = F}
library(lattice)
library(viridisLite)
coul <- viridis(100)
library(lattice)
library(viridisLite)
coul <- viridis(100)
# Part I Multivariate normal distribution
Let $\boldsymbol{x} = (x_1, \dots, x_n), n = 100$ be multivariate normal distributed with $E(x_i) = 0, Var(x_i) = 1$, and $Corr(x_i, x_j) = e^{-0.1|i - j|}$
a) Compute and image the covariance matrix $\boldsymbol{\Sigma}$ of $\boldsymbol{x}$
b) Find the lower Cholesky factor $\boldsymbol{L}$, such that $\boldsymbol{L}\boldsymbol{L}^T = \boldsymbol{\Sigma}$, of this covariance matrix, and image.
c) Sample $\boldsymbol{x} = \boldsymbol{Lz}$, where $\boldsymbol{z}$ is a length n random vector of independent standard normal variables. Plot the sample.
d) Find the precision matrix $\boldsymbol{Q}$ of the covariance matrix, and compute the lower Cholesky factor $\boldsymbol{L}_Q$, such that $\boldsymbol{L}_Q\boldsymbol{L}_Q^T = \boldsymbol{Q}$, of this matrix. Image these matrices and compare them to the images obtained in a) and b)
e) Sample $\boldsymbol{x}$ by solving $\boldsymbol{L}_Q^T\boldsymbol{x} = \boldsymbol{z}$, where $\boldsymbol{z}$ is a length n random vector of independen standard normal variables. Plot the sample.
f) Permute the ordering of variables in $\boldsymbol{x}$, and redo the exercises.
***
## Solution to Part I
### a)
Given that $\boldsymbol{\Sigma} = e^{-0.1|i - j|}$. The covariance matrix can be expressed as follows:
\begin{equation*}
\Sigma = \begin{pmatrix}
1 &  e^{- 0.1 h_{12}} &\dots & e^{- 0.1 h_{1n}} \\
e^{- 0.1 h_{21}} & 1 & \dots & e^{- 0.1 h_{2n}} \\
\vdots & \vdots & \ddots & \vdots \\
e^{- 0.1 h_{n1}} & e^{- 0.1 h_{n2}} & \dots & 1
\end{pmatrix}
\end{equation*}
```{r covariance matrix, echo = F, fig.align='center'}
# set.seed(87012021)
# sizes
n <- 100
# define regular grid of locations
sites1v <- array((1:n),c(n,1))
# compute East and North distances on grid
ww <- array(1,c(n,1))
# Exponential covariance model
Sigma <- exp(-0.1*H)
plotf <- function(val, string){
levelplot(t(val), ylim=c(100,1), col.regions = coul, main = string)
}
# Plot the covariance matrix
plotf(Sigma, "Covariance matrix")
levelplot(t(Sigma),
col.regions = coul, main = "Covariance matrix",
ylim=c(100,1))
# Using R function to calculate Cholesky decomposition
L <- chol(Sigma)
plotf(L, "lower")
# Using R function to calculate Cholesky decomposition
L <- t(chol(Sigma))
plotf(L, "lower")
a = matrix(rnorm(100), nrow = 10, ncol = 10)
a
chol(a\)
chol(a)
source('~/.active-rstudio-document', echo=TRUE)
